{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726ae852",
   "metadata": {
    "id": "726ae852"
   },
   "outputs": [],
   "source": [
    "# Nomes: Ana Carolina Poletto, Bárbara Dapper e Louise Zanol northfleet \n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Coletando múltiplas páginas em loop\n",
    "import time\n",
    "#obtendo a página inicial da wikipedia\n",
    "url = \"https://pt.wikipedia.org/wiki/Wikip%C3%A9dia:P%C3%A1gina_principal\"\n",
    "response = requests. get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#lista de links que estamos pegando\n",
    "links = set()  \n",
    "#refrencia para as paginas que vamos abrir pra pegar os verbetes\n",
    "prox = 0\n",
    "\n",
    "while len(links)< 5000:\n",
    "    #obtendo cada pag\n",
    "    response = requests. get(url)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    #pego todos os links que são parte do corpo do site, já elimino oq não irei usar\n",
    "    todos_links = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "    titulo = soup.select(\".mw-page-title-main\")\n",
    "    for link in todos_links:\n",
    "        #checando se uma tag tem determinada propriedade\n",
    "        #nesse caso estamos vendo se o link é da wikipedia\n",
    "        if \"href\" in link.attrs.keys() and link[\"href\"].startswith(\"/wiki/\"):\n",
    "            #filtrando oq é verbate e oq n é, oq n é verbate tem \":\",\n",
    "            if \":\" in link[\"href\"]:\n",
    "                continue\n",
    "            response = requests. get(\"https://pt.wikipedia.org/\"+link[\"href\"])\n",
    "            soup = BeautifulSoup(response.content)\n",
    "            titulo = soup.select(\".mw-page-title-main\")\n",
    "\n",
    "            #verifcando se o título da página nâo está vazio\n",
    "            if not titulo or not titulo[0].text.strip():\n",
    "                continue\n",
    "\n",
    "            #arruma o nome do arquivo\n",
    "            nome_arquivo = titulo[0].text.strip().replace('\"', '').replace('/', '-').replace(\" \",\"_\")    \n",
    "            conteudo = response.content.decode(\"utf-8\")\n",
    "\n",
    "            #salva no arquivo html\n",
    "            if conteudo:\n",
    "                with open(f\"{nome_arquivo}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(conteudo)\n",
    "                links.add(nome_arquivo)  \n",
    "\n",
    "    #convertendo para lista para pegar um próximo link\n",
    "    url = \"https://pt.wikipedia.org/wiki/\"+ list(links)[prox]\n",
    "    prox += 1\n",
    "    #esperamos meio segundo para fazer uma nova requisção de servidor para não sobrecarregar servido\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"Encontrados {len(links)} nesta página\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a39683",
   "metadata": {},
   "outputs": [],
   "source": [
    " #2.Extraindo informações de Infoboxes\n",
    "import json\n",
    "links = list(links) \n",
    "for link in links:\n",
    "    url = \"https://pt.wikipedia.org/wiki/\" + link\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # pegando o título da página para nomear o arquivo\n",
    "    titulo = soup.find(\"title\").get_text().split(\" – \")[0] \n",
    "\n",
    "    #localizando a infobox\n",
    "    box = soup.find_all(attrs={\"class\": \"infobox infobox_v2\"})\n",
    "    #verifica se encontrou uma infobox\n",
    "    if len(box) > 0:  \n",
    "        #dicionário para armazenar os pares chave-valor\n",
    "        info_dict = {}  \n",
    "        for tag in box[0].find_all(\"tr\"):  \n",
    "            td_tag = tag.find_all(\"td\") \n",
    "            #garante que a linha tem ao menos duas colunas (chave e valor)\n",
    "            if len(td_tag) >= 2: \n",
    "                #usamos join para nâo aparcer as tags do html, somente o texto\n",
    "                chave = \"\".join(filho.get_text(strip=True) if filho.name else str(filho) for filho in td_tag[0].children)\n",
    "                valor = \"\".join(filho.get_text(strip=True) if filho.name else str(filho) for filho in td_tag[1].children)\n",
    "                info_dict[chave] = valor  \n",
    "\n",
    "        #salva os dados em um arquivo json \n",
    "        arquivo = f\"{titulo}.json\"\n",
    "        with open(arquivo, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(info_dict, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e1797-5bb9-433e-99f7-b86d842d323d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
